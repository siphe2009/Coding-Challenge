{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Coding Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using is the Iris dataset, which is commonly used in learning classification. The Iris dataset is a multivariate dataset where each class refers to a type of iris plant. This dataset is free and is publicly available at the UCI Machine Learning Repository.\n",
    "\n",
    "<div>\n",
    "    <img src=https://upload.wikimedia.org/wikipedia/commons/4/49/Iris_germanica_%28Purple_bearded_Iris%29%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg width=300px>\n",
    "</div>\n",
    "\n",
    "This dataset contains a set of 150 records with five attributes - Sepal Length, Sepal Width, Petal Length, Petal Width, and Species. Species is the type of iris plant we will be classifying.\n",
    "\n",
    "Lets import the data to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to pre-processing the data so that we can run it through the classifier. The function should:\n",
    "* Split the data into features and labels\n",
    "* Standardise the features using sklearn's ```StandardScaler```\n",
    "* Split the data into 75% training and 25% testing data.\n",
    "* Use the `train_test_split` method from `sklearn` to do this.\n",
    "* Set random_state to equal 42 for this internal method. \n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take a dataframe as input.\n",
    "* Should return two `tuples` of the form `(X_train, y_train), (X_test, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df):\n",
    "    \n",
    "    dataset = df.copy()\n",
    "    X = dataset.drop('species',axis =1)\n",
    "    y = dataset['species'].values\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X_transform = scaler.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    return ((X_train,y_train),(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.02184904  1.26346019 -1.3412724  -1.31297673]\n",
      " [-0.7795133   2.42047502 -1.2844067  -1.4444497 ]]\n",
      "['Iris-setosa' 'Iris-setosa']\n",
      "[[ 0.31099753 -0.58776353  0.53529583  0.00175297]\n",
      " [-0.17367395  1.72626612 -1.17067529 -1.18150376]]\n",
      "['Iris-versicolor' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = data_preprocess(df)\n",
    "print(X_train[:2])\n",
    "print(y_train[:2])\n",
    "print(X_test[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = data_preprocess(df)\n",
    "print(X_train[:2])\n",
    "print(y_train[:2])\n",
    "print(X_test[:2])\n",
    "print(y_test[:2])\n",
    "```\n",
    "\n",
    "> ```\n",
    "[[-1.02184904  1.26346019 -1.3412724  -1.31297673]\n",
    " [-0.7795133   2.42047502 -1.2844067  -1.4444497 ]]\n",
    "['Iris-setosa' 'Iris-setosa']\n",
    "[[ 0.31099753 -0.58776353  0.53529583  0.00175297]\n",
    " [-0.17367395  1.72626612 -1.17067529 -1.18150376]]\n",
    "['Iris-versicolor' 'Iris-setosa']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Training the Model\n",
    "\n",
    "Now that we have formatted our data, we can fit a model using sklearn's `LogisticRegression` class with its default parameters. Write a function that will take as input `(X_train, y_train)` that we created previously, and return a trained model.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take two numpy `arrays` as input in the form `(X_train, y_train)`.\n",
    "* The returned model should be fitted to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    logic = LogisticRegression()\n",
    "    results = logic.fit(X_train, y_train)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5171756431068295\n",
      "[[-0.71766856  1.32377179 -1.60277458 -1.41952784]\n",
      " [ 0.16218186 -1.28166738  0.51973042 -0.65811392]\n",
      " [ 0.08010274 -0.1281464   1.71205143  2.34828867]]\n"
     ]
    }
   ],
   "source": [
    "lm = train_model(X_train, y_train)\n",
    "print(lm.intercept_[0])\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.round(lm.intercept_, 8), np.array([-1.51717564, -0.85832387, -2.36787217]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "\n",
    "```python\n",
    "lm = train_model(X_train, y_train)\n",
    "print(lm.intercept_[0])\n",
    "print(lm.coef_)\n",
    "```\n",
    "```\n",
    "-1.51717564311\n",
    "[[-0.71766856  1.32377179 -1.60277458 -1.41952784]\n",
    " [ 0.16218186 -1.28166738  0.51973042 -0.65811392]\n",
    " [ 0.08010274 -0.1281464   1.71205143  2.34828867]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained your model, lets see how well it does on the test set. Write a function which returns the accuracy of your trained model when tested with the test set.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take the fitted model and two numpy `arrays` `X_test, y_test` as input.\n",
    "* Should return a `float` of the accuracy of the model. This number should be between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(lm, X_test, y_test):\n",
    "    \n",
    "    y_pred = lm.predict(X_test)\n",
    "    results = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(lm,X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "    \n",
    "```python\n",
    "print(calculate_accuracy(lm,X_test,y_test))\n",
    "```\n",
    ">```\n",
    "0.947368421053\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. Confusion matrices gives us more information on where our model is going wrong - looking specifically at the performance caused by Type I & II errors. Write a function which returns the confusion matrix of your test set.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take the fitted model and two numpy `arrays` `X_test, y_test` as input.\n",
    "* Should return a confusion matrix\n",
    "\n",
    "_**Hint**_ You don't need to do this manually, sklearn has a confusion matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(lm, X_test, y_test):\n",
    "    \n",
    "    y_pred = lm.predict(X_test)\n",
    "    results = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0  9  2]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_matrix(lm,X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "    \n",
    "```python\n",
    "print(conf_matrix(lm,X_test,y_test))\n",
    "```\n",
    ">```\n",
    "[[15  0  0]\n",
    " [ 0  9  2]\n",
    " [ 0  0 12]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which calculates the _multi-class_ Accuracy, Precision, Recall and F1 scores. Recall from your trains that the precision, recall and f1 scores are calculated by\n",
    "\n",
    "$$\n",
    "{\\rm Precision} = \\frac{\\rm TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm Recall} = \\frac{\\rm TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm F1} = 2 \\times \\frac{\\rm Recall \\times Precision}{\\rm Recall + Precision}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/confusion_matrix.png>\n",
    "</div>\n",
    "\n",
    "As per the image above, these scores can be calculated from the elements of either a single column (precision on \"predicted true\") or a single row row (recall on \"actual true\"). \n",
    "\n",
    "Let's generalize this notion to multiple columns. If $i$ represents a row index, and $j$ represents a column index of a confusion matrix $C$ then we can write the recall for the $i^{\\rm th}$ row as \n",
    "$$\n",
    "R_i = \\frac{ C_{ii} }{ \\sum_j C_{ij} },\n",
    "$$\n",
    "the precesion of the $j^{\\rm th}$ column as\n",
    "$$\n",
    "P_j = \\frac{ C_{jj} }{ \\sum_i C_{ij} },\n",
    "$$\n",
    "and the F1 score as\n",
    "$$\n",
    "F_i = 2 \\times \\frac{P_i \\times R_i}{P_i + R_i}.\n",
    "$$\n",
    "\n",
    "Using these, calculate the _average_ recall, precision, and F1 scores for our $3\\times 3$ confusion matrix. As an example, the average recall is $R=\\frac{1}{N}\\sum_i^N R_i$, where $N$ is the number of rows.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take the fitted model and two numpy `arrays` `X_test, y_test` as input.\n",
    "* Should return a tuple in the form (`Accuracy`, `Precision`, `Recall`, `F1-Score`)\n",
    "\n",
    "_**HINT:**_\n",
    "The autograder tests the value of each of these metrics seperately. If you only know how to calculate one metric, then return a tuple with that metric, and zeros for the other metrics. For example, if you only know how to calculate accurcacy, then return a tuple with `(accuracy, 0, 0, 0)`. If you can only calculate the accuracy and recall, then return `(accuracy, 0, recall, 0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(lm, X_test, y_test):\n",
    "    from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return (accuracy,precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.947368\n",
      "Precision: 0.952381\n",
      "Recall: 0.939394\n",
      "F1 score: 0.941026\n"
     ]
    }
   ],
   "source": [
    "(accuracy, precision, recall, f1) = scores(lm, X_test, y_test)    \n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Expected Outputs:**_\n",
    "```python\n",
    "(accuracy, precision, recall, f1) = scores(lm,X_test,y_test)\n",
    "    \n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "```\n",
    "> ```\n",
    "Accuracy: 0.947368\n",
    "Precision: 0.952381\n",
    "Recall: 0.939394\n",
    "F1 score: 0.941026\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
